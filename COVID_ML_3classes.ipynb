{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.19.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.43.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.23.1)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/sai/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Flatten, BatchNormalization, Conv2D, MaxPooling2D, Lambda, Input, AveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "#import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'COVID19', 'NORMAL', 'Viral_Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "image_names=list(os.listdir(\"Database\"))\n",
    "image_names.sort()\n",
    "print(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total Covid images: 1327\n",
      "total Normal images: 1341\n",
      "total Viral_Pneumonia: 1463\n"
     ]
    }
   ],
   "source": [
    "covid_dir=os.path.join(\"Database/COVID19\")\n",
    "norm_dir=os.path.join(\"Database/NORMAL\")\n",
    "pneu_dir=os.path.join(\"Database/Viral_Pneumonia\")\n",
    "\n",
    "\n",
    "print('total Covid images:', len(os.listdir(covid_dir)))\n",
    "print('total Normal images:', len(os.listdir(norm_dir)))\n",
    "print('total Viral_Pneumonia:', len(os.listdir(pneu_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COVID-19 (979).png</td>\n",
       "      <td>Covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVID-19 (580).png</td>\n",
       "      <td>Covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVID-19 (996).png</td>\n",
       "      <td>Covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COVID-19 (646).png</td>\n",
       "      <td>Covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COVID-19 (216).png</td>\n",
       "      <td>Covid19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0 condition\n",
       "0  COVID-19 (979).png   Covid19\n",
       "1  COVID-19 (580).png   Covid19\n",
       "2  COVID-19 (996).png   Covid19\n",
       "3  COVID-19 (646).png   Covid19\n",
       "4  COVID-19 (216).png   Covid19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.DataFrame(os.listdir(covid_dir))\n",
    "df[\"condition\"]= \"Covid19\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Viral_Pneumonia    1463\n",
       "NORMAL             1341\n",
       "Covid19            1327\n",
       "COVID19            1327\n",
       "Name: condition, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.DataFrame.from_records({\"file_name\":os.listdir(covid_dir),\"condition\": \"Covid19\"})\n",
    "\n",
    "for f in image_names[1:]:\n",
    "    folder_path=\"Database/\" + f\n",
    "    temp_df= pd.DataFrame.from_records({\"file_name\":os.listdir(folder_path),\"condition\": f })\n",
    "    df=df.append(temp_df)\n",
    "    \n",
    "df[\"condition\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2893 images belonging to 3 classes.\n",
      "Found 1238 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "img_width, img_height=200,200\n",
    "batch_size=128\n",
    "\n",
    "data_dir = \"Database/\"\n",
    "\n",
    "\n",
    "datagen= ImageDataGenerator(rescale=1/255,validation_split=.3,rotation_range=20,\n",
    "                           shear_range=.2,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.2)\n",
    "\n",
    "\n",
    "train_generator= datagen.flow_from_directory(\n",
    "                data_dir,\n",
    "                target_size=(img_width,img_height),\n",
    "                batch_size=batch_size,\n",
    "                subset=\"training\",\n",
    "                class_mode=\"categorical\", \n",
    "                classes= [\"COVID19\", \"NORMAL\",\"Viral_Pneumonia\"],\n",
    "                shuffle=True, seed=30)\n",
    "\n",
    "\n",
    "datagen2=ImageDataGenerator(rescale=1/255,validation_split=.3)\n",
    "\n",
    "test_generator=datagen2.flow_from_directory(\n",
    "                data_dir,\n",
    "                target_size=(img_width,img_height),\n",
    "                batch_size=batch_size,\n",
    "                classes= [\"COVID19\", \"NORMAL\",\"Viral_Pneumonia\"],\n",
    "                class_mode=\"categorical\", subset=\"validation\", shuffle=True, seed=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "### Using new dataset and with 3 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 11:46:12.955337: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifier=Sequential()\n",
    "classifier.add(Conv2D(32, kernel_size=3, activation=\"relu\", input_shape=(200,200,3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(strides=(2,2)))\n",
    "classifier.add(Dropout(.3))\n",
    "\n",
    "classifier.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(strides=(2,2)))\n",
    "classifier.add(Dropout(.5))\n",
    "\n",
    "classifier.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(strides=(2,2)))\n",
    "classifier.add(Dropout(.4))\n",
    "\n",
    "classifier.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(strides=(2,2)))\n",
    "classifier.add(Dropout(.3))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(512,activation=\"relu\"))\n",
    "classifier.add(Dense(128,activation=\"relu\"))\n",
    "classifier.add(Dropout(.4))\n",
    "\n",
    "classifier.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 198, 198, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 198, 198, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 99, 99, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 99, 99, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 97, 97, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 97, 97, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 46, 46, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 46, 46, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 23, 23, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 23, 23, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 21, 21, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 21, 21, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 10, 10, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 10, 10, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               3277312   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,409,699\n",
      "Trainable params: 3,409,315\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=\"adam\", \n",
    "              metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "total_sample=train_generator.n\n",
    "total_test=test_generator.n\n",
    "batch_size=128\n",
    "\n",
    "history= classifier.fit_generator(\n",
    "            train_generator, steps_per_epoch=int(total_sample/batch_size),\n",
    "            epochs=30,\n",
    "            validation_data=test_generator, \n",
    "            validation_steps=int(total_test/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_1.h5')\n",
    "\n",
    "train_loss, train_acc = classifier.evaluate(train_generator)\n",
    "print(\"\\n Train Accuracy:\", train_acc)\n",
    "print(\"\\n Train Loss:\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(model_1.h5, 'rb'))\n",
    "rtrain_loss, train_acc = loadedmodel.evaluate(train_generator)\n",
    "print(\"\\n Train Accuracy:\", train_acc)\n",
    "print(\"\\n Train Loss:\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc= classifier.evaluate(test_generator)\n",
    "print(\"\\n Test Accuracy:\", test_acc)\n",
    "print(\"\\n Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('Model Accuracy per Epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Accuracy per Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fs/rgrb722x0vxb50q4vybz6rnh0000gn/T/ipykernel_9924/4102652020.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save history of model into csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhist_df1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# or save to csv:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhist_csv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'history1.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Save history of model into csv\n",
    "hist_df1 = pd.DataFrame(history.history) \n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'history1.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df1.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier= Sequential()\n",
    "\n",
    "classifier.add(Conv2D(32, (3,3), input_shape=(200,200,3),activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Conv2D(64,(3,3),activation=\"relu\"))\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "\n",
    "classifier.add(Dense(units = 128 , activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "\n",
    "classifier.add(Dense(units = 3 , activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 198, 198, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 198, 198, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 99, 99, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 97, 97, 64)        18496     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 97, 97, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 97, 97, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 48, 48, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 46, 46, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 46, 46, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 23, 23, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 21, 21, 128)       73856     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 21, 21, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 21, 21, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 10, 10, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               262272    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 542,083\n",
      "Trainable params: 541,251\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fs/rgrb722x0vxb50q4vybz6rnh0000gn/T/ipykernel_9924/2057428070.py:13: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history= classifier.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 3/22 [===>..........................] - ETA: 2:30 - loss: 2.2098 - accuracy: 0.3724"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fs/rgrb722x0vxb50q4vybz6rnh0000gn/T/ipykernel_9924/2057428070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m history= classifier.fit_generator(\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_sample\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m         stacklevel=2)\n\u001b[0;32m-> 2016\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   2017\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier.compile(\n",
    "  optimizer='adam',\n",
    "  loss=\"categorical_crossentropy\",\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "total_sample= train_generator.n\n",
    "total_test=test_generator.n\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "history= classifier.fit_generator(\n",
    "            train_generator, steps_per_epoch=int(total_sample/batch_size),\n",
    "            epochs=10,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=int(total_test/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"model_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = classifier.evaluate(train_generator)\n",
    "print(\"\\n Train Accuracy:\", train_acc)\n",
    "print(\"\\n Train Loss:\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "23/23 [==============================] - 121s 5s/step - loss: 6.5042 - categorical_accuracy: 0.3211\n",
      "\n",
      " Train Accuracy: 0.32111993432044983\n",
      "\n",
      " Train Loss: 6.504210948944092\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('model_2.h5')\n",
    " summarize model.\n",
    "model.summary()\n",
    "train_loss, train_acc = classifier.evaluate(train_generator)\n",
    "print(\"\\n Train Accuracy:\", train_acc)\n",
    "print(\"\\n Train Loss:\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Loss/ Test Accuracy\n",
    "test_loss, test_acc= classifier.evaluate(test_generator)\n",
    "print(\"\\n Test Accuracy:\", test_acc)\n",
    "print(\"\\n Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT ACCURACY\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model 2 Accuracy per Epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSS\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model 2 Accuracy per Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history of model into csv\n",
    "hist_df2 = pd.DataFrame(history.history) \n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'history2.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df2.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3\n",
    "# GENERATE MODEL\n",
    "classifier= Sequential()\n",
    "\n",
    "# First convolution layer\n",
    "classifier.add(Conv2D(32, (3,3), input_shape=(200,200,3),activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Second convolution layer\n",
    "classifier.add(Conv2D(32,(3,3),activation=\"relu\"))\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Flatten the results to feed into a dense layer\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# 128 neuron in the fully-connected layer\n",
    "classifier.add(Dense(128 , activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "\n",
    "# 14 output neurons for 14 classes with the softmax activation\n",
    "classifier.add(Dense(3 , activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3, with augmentation\n",
    "classifier.compile(\n",
    "  optimizer='adam',\n",
    "  loss=\"categorical_crossentropy\",\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# total_valsample=test_generator.n\n",
    "total_sample= train_generator.n\n",
    "total_test=test_generator.n\n",
    "batch_size=128\n",
    "\n",
    "history= classifier.fit_generator(\n",
    "            train_generator, steps_per_epoch=int(total_sample/batch_size),\n",
    "            epochs=10,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=int(total_test/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model-save weights\n",
    "classifier.save(\"model_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss/ Train Accuracy\n",
    "train_loss, train_acc = classifier.evaluate(train_generator)\n",
    "print(\"\\n Train Accuracy:\", train_acc)\n",
    "print(\"\\n Train Loss:\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Loss/ Test Accuracy\n",
    "test_loss, test_acc= classifier.evaluate(test_generator)\n",
    "print(\"\\n Test Accuracy:\", test_acc)\n",
    "print(\"\\n Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT ACCURACY\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model 3 Accuracy per Epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSS\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model 3 Accuracy per Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history of model into csv\n",
    "hist_df3 = pd.DataFrame(history.history) \n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'history3.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df3.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4- Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height=200,200\n",
    "batch_size=128\n",
    "\n",
    "data_dir = \"Database/\"\n",
    "\n",
    "# Rescale images \n",
    "datagen= ImageDataGenerator(rescale=1/255,validation_split=.3,rotation_range=20,\n",
    "                           shear_range=.2,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.2,\n",
    "                           preprocessing_function=preprocess_input)\n",
    "\n",
    "# Flow training images in batches of 128 using train_data \n",
    "train_generator= datagen.flow_from_directory(\n",
    "                data_dir,\n",
    "                target_size=(64,64),\n",
    "                batch_size=batch_size,\n",
    "                subset=\"training\",\n",
    "                class_mode=\"categorical\", \n",
    "                classes= [\"COVID19\", \"NORMAL\",\"Viral_Pneumonia\"],\n",
    "                shuffle=True, seed=30)\n",
    "\n",
    "\n",
    "datagen2=ImageDataGenerator(rescale=1/255,validation_split=.3,\n",
    "                            preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator=datagen2.flow_from_directory(\n",
    "                data_dir,\n",
    "                target_size=(64,64),\n",
    "                batch_size=batch_size,\n",
    "                classes= [\"COVID19\", \"NORMAL\",\"Viral_Pneumonia\"],\n",
    "                class_mode=\"categorical\", subset=\"validation\", shuffle=True, seed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4\n",
    "\n",
    "datagen= ImageDataGenerator(rescale=1/255,validation_split=.3)\n",
    "\n",
    "classifier=VGG16(weights=\"imagenet\", include_top=False, input_shape=(64,64,3))\n",
    "\n",
    "for layer in classifier.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "x=Flatten()(classifier.output)\n",
    "x=Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "model=Model(inputs=classifier.input, outputs=x)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sample= train_generator.n\n",
    "total_test=test_generator.n\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "\n",
    "history= model.fit_generator(\n",
    "            train_generator, steps_per_epoch=int(total_sample/batch_size),\n",
    "            epochs=100,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=int(total_test/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model-save weights\n",
    "classifier.save(\"model_4tf_vgg16.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss/ Train Accuracy\n",
    "train_loss, train_acc = model.evaluate(train_generator)\n",
    "print(\"\\n Train Accuracy:\", train_acc)\n",
    "print(\"\\n Train Loss:\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Loss/ Test Accuracy\n",
    "test_loss, test_acc= model.evaluate(test_generator)\n",
    "print(\"\\n Test Accuracy:\", test_acc)\n",
    "print(\"\\n Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT ACCURACY\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model 3 Accuracy per Epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSS\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model 3 Accuracy per Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history of model into csv\n",
    "hist_df4 = pd.DataFrame(history.history) \n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'history4_tf_vgg16.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df4.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5-Transfer Learning\n",
    "### Same as Model 4 but at 200 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5- Transfer Learning\n",
    "data_dir = \"Database/\"\n",
    "batch_size=128\n",
    "\n",
    "# Rescale images \n",
    "datagen= ImageDataGenerator(rescale=1/255,validation_split=.3,rotation_range=20,\n",
    "                           shear_range=.2,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.2,\n",
    "                           preprocessing_function=preprocess_input)\n",
    "\n",
    "# Flow training images in batches of 128 using train_data \n",
    "train_generator= datagen.flow_from_directory(\n",
    "                data_dir,\n",
    "                target_size=(64,64),\n",
    "                batch_size=batch_size,\n",
    "                subset=\"training\",\n",
    "                class_mode=\"categorical\", \n",
    "                classes= [\"COVID19\", \"NORMAL\",\"Viral_Pneumonia\"],\n",
    "                shuffle=True, seed=30)\n",
    "\n",
    "\n",
    "datagen2=ImageDataGenerator(rescale=1/255,validation_split=.3,\n",
    "                            preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator=datagen2.flow_from_directory(\n",
    "                data_dir,\n",
    "                target_size=(64,64),\n",
    "                batch_size=batch_size,\n",
    "                classes= [\"COVID19\", \"NORMAL\",\"Viral_Pneumonia\"],\n",
    "                class_mode=\"categorical\", subset=\"validation\", shuffle=True, seed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen= ImageDataGenerator(rescale=1/255,validation_split=.3)\n",
    "\n",
    "# classifier=VGG16(weights=\"imagenet\", include_top=False, input_shape=(64,64,3))\n",
    "\n",
    "\n",
    "# x=Flatten()(classifier.output)\n",
    "# x=Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "classifier = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(64, 64, 3)))\n",
    "\n",
    "x = classifier.output\n",
    "# x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten(name=\"flatten\")(x)\n",
    "# x = Dense(128, activation=\"relu\")(x)\n",
    "# x = Dropout(0.6)(x)\n",
    "x = Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "for layer in classifier.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "model=Model(inputs=classifier.input, outputs=x)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sample= train_generator.n\n",
    "total_test=test_generator.n\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "history= model.fit_generator(\n",
    "            train_generator, \n",
    "            steps_per_epoch=int(total_sample/batch_size),\n",
    "            epochs=200,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=int(total_test/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model-save weights\n",
    "classifier.save(\"model_5tf_vgg16.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss/ Train Accuracy\n",
    "train_loss, train_acc = model.evaluate(train_generator)\n",
    "print(\"\\n Train Accuracy:\", train_acc)\n",
    "print(\"\\n Train Loss:\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Loss/ Test Accuracy\n",
    "test_loss, test_acc= model.evaluate(test_generator)\n",
    "print(\"\\n Test Accuracy:\", test_acc)\n",
    "print(\"\\n Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT ACCURACY\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model 5 Accuracy per Epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSS\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model 5 loss per Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history of model into csv\n",
    "hist_df5 = pd.DataFrame(history.history) \n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'history5_tf_vgg16.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df5.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(\n",
    "#     classifier, to_file='vgg16.png', show_shapes=False,\n",
    "#     show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 3, 3, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3, 3, 224)         114912    \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2016)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3)                 6051      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,835,651\n",
      "Trainable params: 120,963\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "23/23 [==============================] - 121s 5s/step - loss: 6.4946 - categorical_accuracy: 0.3211\n",
      "\n",
      " Train Accuracy: 0.32111993432044983\n",
      "\n",
      " Train Loss: 6.494606971740723\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('VGG16_model.h5')\n",
    "\n",
    "model.summary()\n",
    "train_loss, train_acc = classifier.evaluate(train_generator)\n",
    "print(\"\\n Train Accuracy:\", train_acc)\n",
    "print(\"\\n Train Loss:\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
